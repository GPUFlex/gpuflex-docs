# Training Workflow

## Sequence diagram

<figure><img src="../.gitbook/assets/Untitled diagram _ Mermaid Chart-2025-07-16-142626.png" alt=""><figcaption></figcaption></figure>

## Explanation

This section outlines explanation of the sequence of events during a GPUFlex training job, from the moment a user initiates a task to the final model output:

1. **Job Submission (Consumer Action):** \
   The process begins when a consumer defines a training job through the GPUFlex interface. The user specifies the details of the task – for example, uploading their training dataset (or pointing to a data source), choosing a model architecture or base model to fine-tune, and setting training parameters (like number of epochs, learning rate, etc.). Once the user confirms the job configuration and funding, the job request is broadcast to the GPUFlex network.\

2. **Job Queuing and Allocation:** \
   The dispatcher receives the new job request and places it in the scheduling queue. The dispatcher component evaluates the requirements of the job – e.g. the size of the dataset, the complexity of the model, and any specific hardware needs (like a GPU with certain memory). It then determines how many and what kind of worker nodes are needed. The dispatcher locks the user’s payment in escrow (to guarantee rewards for node operators) and marks the job as pending execution.\

3. **Shard Preparation (Data Partitioning):** \
   Before workers are assigned, the sharding backend kicks in to partition the workload. The dataset is split into shards or subsets that can be processed independently. For instance, if there are N worker GPUs available, the dataset might be divided into N roughly equal parts. The sharding backend also packages any training instructions needed for each shard (such as the range of data indices, or if using pre-defined data splits). In this step, an initial global model is prepared as well – either a random initialization or a provided pre-trained model that all workers will start training from. The dispatcher now has a list of tasks (shards) and the initial model ready to distribute.\

4. **Worker Selection:** \
   The dispatcher selects a set of available worker nodes to execute the training. It looks at the pool of online producers (GPU nodes) and filters by those that meet the criteria (for example, a node must have enough VRAM to load the model and data shard). Other factors like node reliability (XP score), geographic location (for data locality considerations), or performance history might also be considered. Suppose the job needs N shards – the dispatcher will choose N workers (plus maybe a few spares for redundancy) from the pool. Each chosen worker is then sent a proposal or assignment for the job. The nodes acknowledge if they are ready to take the task.\

5. **Dispatching Tasks to Workers:** \
   Once the workers are confirmed, the dispatcher assigns each worker a shard of data and sends them the necessary payload to start training. This typically includes: the initial model parameters (so all workers start from the same model state), the data shard or instructions on how to fetch it (e.g. a download link or dataset chunk), and the training hyperparameters specific to that shard (such as number of local epochs to run, batch size, etc.). The dispatcher might transmit a container image or specify an environment so that each worker runs the training code in a consistent setup. At this point, each worker node has what it needs to begin training its portion of the job.\

6. **Parallel Training on Worker Nodes:** \
   All selected worker nodes now execute the training process concurrently on their respective data shards. Each worker loads the model and data chunk into its GPU and performs the training computations (forward passes, backward propagation, weight updates) for the allocated number of steps or epochs on that shard. During this process, workers typically operate independently of one another – they are crunching through different portions of data in parallel, which drastically speeds up overall training time. The workers utilize their full GPU capacity for the task, and the GPUFlex container environment ensures the code runs sandboxed and does not interfere with the node’s system. While training, each worker regularly sends heartbeat signals or progress updates to the dispatcher (for example, “X% of the shard processed” or simple alive pings). If a worker encounters an error or slows down significantly, the dispatcher can decide to redistribute that shard’s work to another node (maintaining robust execution).\

7. **Local Training Completion:** \
   After completing the local training pass (say each worker finished its epoch on the shard), each worker node finalizes its updated model. At this point, the model on that worker has learned from that worker’s specific data subset. The worker now prepares a report to send back. Instead of sending the entire trained model file (which could be large), in many cases the worker sends just the _model weight updates_ or differences from the initial model, which is more network-efficient. Optionally, the worker might also compute some evaluation metrics on its local data (like the training loss or accuracy on that shard) to include with the update. Each worker then transmits its results (the updated weights or gradient and any metrics) back to the aggregator node or service.\

8. **Aggregation of Model Updates:** \
   Once the central aggregator (aggregator module) receives the updates from all active workers, it performs the aggregation step. The typical method is to calculate a weighted average of all the workers’ model parameters – this is the Federated Learning approach known as Federated Averaging. Essentially, the aggregator merges the knowledge from each shard back into a single global model. For example, if one worker processed 1 million samples and another processed 500k, the first worker’s update might be given double the weight in the average. The aggregator combines all contributions to produce a new set of model parameters that reflect training on the entire dataset (as if it had been done on one machine). As part of this step, the aggregator may also validate the updates (checking for any anomalies or outliers) to ensure no worker sent a malicious or erroneous update. After aggregation, the single updated model is now the current global model.\

9. **Iteration (Multiple Rounds):** \
   The dispatcher checks if the training objective is met or if multiple training rounds are required. In many cases, one round of federated averaging (one epoch on each shard) is not enough to fully train the model. If the job calls for multiple epochs, the newly aggregated model from step 8 is then redistributed to the workers for a next round. The dispatcher instructs the workers to load this updated global model and continue training on their data for another pass (possibly with a new data shard or the same shard for multiple epochs, depending on the plan). Steps 6 through 8 repeat for each round: workers train further on their data, and the aggregator combines the updates. Throughout these iterations, the model gradually improves as it sees more data from all shards. This loop continues until a stopping condition is reached – for example, the specified number of epochs are completed, or the model’s validation accuracy has hit the target, or the user’s budget is exhausted.\

10. **Finalization of Training:** \
    When the training rounds are complete, the aggregator produces the final version of the trained model. The dispatcher marks the job as finished. The final model (the set of learned weights) is then delivered back to the consumer. The user can download the model artifact (which could be a file containing the neural network weights or any specified output format). The platform may also provide a training report summarizing metrics like final accuracy or loss, and logs from the training process for transparency. At this stage, the user’s goal is achieved – they have a trained model ready for deployment or further evaluation.\

11. **Reward Distribution and Cleanup:** \
    With the job successfully completed, GPUFlex handles the payout and resets the system for the next tasks. The network updates the XP (experience points) of those worker nodes to reflect the completed job – increasing their XP as a reward for good performance. Any temporary data (such as shards stored on nodes) can be cleaned up as per policy to free resources. Workers then become available to take on new jobs, and the consumer can retrieve their model and leave feedback if applicable.\


Each of these steps is orchestrated seamlessly by the GPUFlex platform, abstracting away the complexity for the end-user. From the user’s perspective, they simply see their model being trained much faster and cheaper than it would on a single machine, thanks to the distributed power of the network. Internally, however, this workflow ensures that dozens or hundreds of independent components work in concert to produce a correct, verified result in a trustless environment.

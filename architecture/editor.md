# System Architecture

## Major Components

### Consumer (Client)

This is the end-user who submits a training job. The consumer uses a client application (for example, a web portal or CLI) to specify the training task – uploading the dataset or providing a data source, selecting a model or training code, and setting parameters like epochs or budget. The consumer’s client communicates with the GPUFlex network by sending a job request to the dispatcher.&#x20;

### Dispatcher (Coordinator)

The dispatcher is the brain of the operation, responsible for orchestrating training jobs across the network. It receives incoming job requests from consumers and manages a queue of jobs. For each job, the dispatcher determines how to break the work into subtasks (with help from the sharding backend) and which worker nodes should run them. It keeps a registry of available worker nodes along with their hardware specs (GPU type, memory, current load, network speed, etc.) and possibly their reputation/XP. The dispatcher matches jobs to workers that meet the requirements (e.g. enough VRAM for the model) and handles scheduling. It communicates with workers to dispatch training tasks, sends them the model initialization and data shard assignments, and monitors progress via a heartbeat protocol (workers regularly report their status). If a worker fails or goes offline, the dispatcher can reassign that task to another node to ensure the job continues. The dispatcher is also integrated with the blockchain layer: it interacts with smart contracts to deposit the consumer’s payment, and later triggers payouts to workers upon job completion. In essence, the dispatcher coordinates all moving parts in real-time, ensuring the distributed training runs smoothly from start to finish.

While recovery from failures is currently handled by reassigning tasks, potential future implementations may leverage federated learning algorithms to save and resume the current state of partially trained models, enabling more efficient continuation rather than restarting from scratch. However, this feature is still complex to implement reliably across distributed nodes and is not part of the current system.

### Sharding Module (Data/Task Partitioning)

The sharding backend is module that prepares the training workload for distribution. When a new job comes in, this backend takes the user’s dataset and slices it into smaller shards or subsets, and also determines the training plan for each shard. For example, if the dataset has 10 million samples and 10 worker GPUs are available, the backend might partition it into 10 chunks of 1 million samples each. It ensures shards are created in a balanced way (e.g. stratifying data so each shard is representative if needed). The sharding backend also may package the training code or model so that it can run in isolated pieces – for instance, setting up configuration so each worker knows which epochs or batch range to process.&#x20;

In some implementations, this involves preparing containerized environments or scripts specific to each shard. The dispatcher leverages this backend to get a list of tasks (shards) which it then assigns to worker nodes.&#x20;

In summary, the sharding module handles the data distribution logic: who trains on what portion of data (or model), and it provides the necessary metadata so that results from all shards can later be combined correctly.

### Worker Nodes (Producers)

The worker nodes are the distributed GPUs contributed by participants around the world. Each worker runs the GPUFlex node software (desktop app), which allows it to receive tasks, execute training, and communicate results. When the dispatcher assigns a training subtask to a worker, the node downloads the necessary data shard (unless it was pre-sent or accessed via a shared storage) and the current model parameters. Workers run the training computation locally – typically inside a container or sandbox that contains the ML framework (e.g. PyTorch/TensorFlow) and the model code. They train on the data shard for the specified number of iterations (for example, a certain number of epochs or steps) as directed by the task. During training, the worker reports back periodic heartbeats to the dispatcher (to confirm it’s alive and give progress metrics). Once the worker finishes its assigned training round or epoch, it will upload its results: usually this is an updated set of model weights (or gradients) and possibly evaluation metrics computed on that shard. Workers then await further instructions – the dispatcher might send a new global model for the next round if the training continues iteratively, or inform the worker that the job segment is complete. In return for their work, worker nodes earn points for each completed task. The node software ensures that upon submitting proof of work (the model update), the node’s contribution is logged for reward. Multiple workers often work in parallel on the same job (each on different shards), so the network significantly speeds up training by utilizing all available GPUs concurrently.

### Aggregation & Finalization (Model Merger)

&#x20;GPUFlex employs a central aggregation mechanism – sometimes called the finalization cloud – to collect and merge the results from all the workers. After each training round, the aggregator (which can be a dedicated server or service) waits for the model updates from the various worker nodes. It then combines these updates into a single consistent global model. In practice, this is done via algorithms like Federated Averaging, where the parameters from each worker’s model are averaged together (often weighted by the number of samples each worker processed). The aggregator produces a new global model checkpoint that incorporates the contributions of all nodes. This updated model may be sent out again to the workers for a next round of training (if multiple iterations are needed), or if the training is complete, the final model is produced. The finalization stage also involves verification: ensuring that the updates received are valid and not malicious (see _Aggregation and Verification_ section for details). Once the final model is ready, the aggregator triggers the completion of the job – the trained model is passed back to the consumer (e.g. the file is made available for download), and the smart contract is instructed to release payments. The aggregator thus closes the loop of the training session. In the current architecture, the aggregator is a logically centralized component (for accuracy and efficiency), but it can be hosted in a robust, decentralized manner (e.g. by a consortium of nodes or a secure multi-party computation) to avoid any single point of trust. It communicates closely with the dispatcher (or might be part of the dispatcher service) to coordinate each round of training.

### Communication Flow

Putting it all together, a typical job proceeds as follows – The consumer’s request goes to the dispatcher, which breaks the job into shard tasks via the sharding backend. The dispatcher assigns shards to multiple workers in parallel and sends them the initial model and data references. Workers perform training on their respective shards and send back model updates. The aggregator merges those updates into a new model. If the job requires more epochs, the dispatcher (with the aggregator) will redistribute the updated model for another round of shard training. This cycle repeats until completion criteria are met. Finally, the completed model is delivered to the consumer, and payments are automatically distributed to the contributors. Throughout the process, the blockchain-based incentive layer tracks contributions (for XP and token rewards) and escrows the funds, ensuring a transparent and trustless coordination of the entire system.

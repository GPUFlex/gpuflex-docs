# Dispatcher

The dispatcher is the coordinating node of the GPUFlex system, essentially acting as the distributed job scheduler and network orchestrator. It carries out a variety of critical functions to ensure jobs are executed efficiently and reliably.

## Job Scheduling

The dispatcher maintains a queue of incoming training jobs from consumers. If multiple jobs are submitted around the same time, the dispatcher may prioritize them (for example, first-come-first-served, or based on token bids if a market-driven scheduling is used). It keeps track of the resources required for each job (e.g. “Job #101 needs 4 GPUs for \~2 hours”) and the resources currently available in the network. The dispatcher decides when to start a new job versus hold it in queue, ensuring that there are enough idle worker nodes to handle it without causing excessive slowdown to ongoing jobs. In a scenario where jobs can run in parallel, the dispatcher will allocate different subsets of workers to different jobs, effectively time-sharing the network’s resources.

## Node Matching and Assignment

A core task of the dispatcher is matching each training job (or each training subtask) to appropriate worker nodes. It uses the information each worker advertises (GPU specs, availability, location, XP/reliability score) to find a good fit. For example, if a job requires high memory (say a large model), the dispatcher will only consider nodes with GPUs above that VRAM threshold. It also tries to optimize the overall usage: if there are very fast nodes and very slow nodes, it might group nodes of similar speed on the same job to avoid one straggler delaying the whole round. The dispatcher sends out task assignments to selected workers, including all necessary info to start (which we detailed in the workflow). For large jobs that use many nodes, the dispatcher might break it down further – e.g., assign 8 nodes now and then another 8 when they free up, if not enough are idle at once. It keeps a directory of which node is working on which job and shard at any time.

## Resource Monitoring & Heartbeats

Once workers are running, the dispatcher continuously monitors the health of the job through heartbeat signals. Each worker node pings the dispatcher at a regular interval (e.g., every few seconds) with a brief status. The dispatcher aggregates these: if all heartbeats are arriving normally, it knows things are on track. If a heartbeat from a particular worker is missed or indicates an error, the dispatcher flags that node as problematic. It might attempt a quick retry (maybe ping again or ask the node if it’s okay) but if the node has indeed gone offline or crashed, the dispatcher will mark it as failed. The heartbeat protocol also often carries progress info (like “node X is 60% done with its current epoch”). The dispatcher can use this info to identify slow nodes – if one node is far behind others, it may predict a delay and could decide to assign an additional node to help, or simply note it for future scheduling (like maybe avoid giving that node too large a shard next time).

## Fault Tolerance and Redundancy

When something goes wrong, the dispatcher springs into corrective action. If a worker fails mid-job, the dispatcher will attempt to reassign that work to another available node. For example, if node 7 died at 50% progress of its shard, the dispatcher could either restart that shard from scratch on a new node, or if periodic checkpoints are saved, send the latest checkpoint to a new node to resume from mid-way. This way, the overall job can still complete, perhaps with a minor delay. The dispatcher also implements redundancy strategies as needed: it may assign two nodes the same shard from the outset for important jobs. In that case, if one fails, the other likely still completes the shard. Or both complete and their results can be cross-checked for verification. The dispatcher carefully manages these duplicates to ensure efficiency – it won’t double-compute everything by default (due to cost), but it has the ability to overlap work for the sake of reliability or trust. In essence, the dispatcher is responsible for delivering the “exactly-once or at-least-once” execution guarantee to the user despite an unreliable pool of workers. It keeps track of which shards are done, which are pending, and reassigns as necessary until all pieces of the job are successfully processed.

## Coordination with Aggregation

The dispatcher and the aggregation component work in tandem. In some implementations they might even be part of the same server or process. After distributing tasks to workers, the dispatcher knows to expect results back. It will collect model updates from all the workers (either directly or via the aggregator) and determine when a round is complete (i.e., all shard results are in or have been accounted for). Once a round’s results are aggregated into a new model, the dispatcher oversees sending out that new model to workers for the next round. Essentially, the dispatcher orchestrates the loop of distribute -> collect -> aggregate -> redistribute, until the job is done. It ensures that all participants move in lockstep through the training cycles.

## Record-Keeping

The dispatcher also interfaces with the blockchain or accounting layer to handle payments and stats. When a job finishes, the dispatcher (or an associated payment module) calculates the reward split for each worker. It updates each worker’s XP and maintains logs of the job (for accountability and auditing). Additionally, the dispatcher might log performance data about workers – for example, node A was 20% faster than node B – which can feed back into future scheduling decisions or user analytics. In a decentralized future, the dispatcher’s role in payment might be minimized in favor of automated smart contracts, but initially it often has a direct hand in initiating the payout transactions once results are verified.

## Scalability and Deployment

Practically, the dispatcher is a server (or cluster of servers) that needs to be highly reliable and responsive, since it’s coordinating many moving parts. It’s like the “traffic controller” of GPUFlex. In early stages, it might be run by the GPUFlex team as a centralized service for simplicity. Over time, the dispatcher itself could be decentralized or distributed (for example, running a consensus among multiple dispatcher nodes or using a blockchain-based scheduling mechanism). Regardless of deployment, its logic remains: manage the pool of work and workers. The dispatcher must scale as the network grows – it should handle hundreds or thousands of workers and many simultaneous jobs. Technologies like message queues, databases for tracking state, and possibly P2P discovery protocols (if making it decentralized) are used under the hood.\


In summary, the dispatcher is the mastermind that keeps the GPUFlex network efficient and trustworthy. It juggles incoming tasks, finds the right machines to do them, keeps everybody in sync, handles hiccups, and finally makes sure everyone gets paid and the user gets their model. Without the dispatcher’s intelligent oversight, a group of random GPUs would not seamlessly become a coherent distributed training cluster – the dispatcher provides that coherent organization.

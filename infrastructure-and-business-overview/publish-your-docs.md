# Infrastructure Strategy

A cornerstone of this business model is the distribution of infrastructure between external providers and our own platform: roughly 85-90% of GPU compute power will be user-hosted, and 10-15% will be owned by the platform. This ratio strikes a balance between fully decentralized marketplace and company-owned cloud approach:

### User-Hosted – Decentralized Marketplace:

&#x20;The majority of our compute capacity comes from third-party hosts who register their GPUs on our platform. In practice, anyone with spare GPU – from individual enthusiasts to small cloud farms – can list their hardware for rent. This crowdsourced infrastructure brings immense diversity in hardware (consumer GPUs like RTX cards, professional cards like A100/H100, etc.) and locations. For our platform, that means customers will be matched with the perfect GPU type and region for their needs, enhancing flexibility. The benefit for scalability and cost is clear: we tap into existing hardware worldwide, expanding capacity without huge capital investment to the infrastructure, and competition among hosts drives prices down.

### Platform-Owned – Managed Core Servers:

&#x20;In addition to the community pool, we will operate our own fleet of high-end GPU servers. Our owned infrastructure will be relatively small (to stay lean on costs), but critical for control. These machines will ensure a baseline of availability and performance even if user-supplied nodes fluctuate. They also let us enforce uniform standards (network speed, security protocols, maintenance schedules) on a portion of the platform. Customers with sensitive or large-scale jobs might opt to run on these “platform-certified” servers for maximum reliability. Having a dedicated core of hardware means we can offer enterprise features similar to traditional cloud providers: e.g. guaranteed uptime SLAs, fast inter-node networking, and advanced scheduling across these nodes. In essence, this 15% acts as the reliability anchor of our service. It also gives us leverage to pilot new features (like improved orchestration or specialized hardware) on our own servers before rolling out widely.\


By orchestrating these two pools together, we ensure users get the best of both worlds. For instance, a user could choose a lowest-cost option from the community pool for a quick experiment, then be switched to the platform-owned nodes for a mission-critical job that demands guaranteed throughput. Our scheduler and interface will abstract the complexity, so users see one unified cloud. We will implement a rating/reputation system and minimum hardware standards, ensuring that even the external hosts meet reliability benchmarks. Overall, this infrastructure mix is designed to maximize capacity and cost efficiency (via community GPUs) while still providing controlled, high-quality service (via our own GPUs). It is a scalable model that can grow rapidly with user contributions, yet remain grounded by a core of well-maintained servers.
